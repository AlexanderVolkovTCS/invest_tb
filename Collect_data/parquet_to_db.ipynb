{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b7ca64-ff8f-451c-bac7-666095db7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/07 05:11:41 WARN Utils: Your hostname, nikolay resolves to a loopback address: 127.0.1.1; using 192.168.43.45 instead (on interface wlp2s0)\n",
      "22/05/07 05:11:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/nikolay/anaconda3/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/nikolay/anaconda3/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/nikolay/.ivy2/cache\n",
      "The jars for the packages stored in: /home/nikolay/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fe0321f7-40ae-4b59-a513-74168ea9e03a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.2.5 in central\n",
      ":: resolution report :: resolve 216ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\torg.postgresql#postgresql;42.2.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fe0321f7-40ae-4b59-a513-74168ea9e03a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/9ms)\n",
      "22/05/07 05:11:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "22/05/07 05:11:43 WARN DependencyUtils: Local jar /home/nikolay/Projects/tinkoff-invest/Collect_data/postgresql-42.2.5.jar does not exist, skipping.\n",
      "22/05/07 05:11:43 INFO SparkContext: Running Spark version 3.2.1\n",
      "22/05/07 05:11:43 INFO ResourceUtils: ==============================================================\n",
      "22/05/07 05:11:43 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "22/05/07 05:11:43 INFO ResourceUtils: ==============================================================\n",
      "22/05/07 05:11:43 INFO SparkContext: Submitted application: tinkof-invest\n",
      "22/05/07 05:11:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 20480, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "22/05/07 05:11:43 INFO ResourceProfile: Limiting resource is cpu\n",
      "22/05/07 05:11:43 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "22/05/07 05:11:43 INFO SecurityManager: Changing view acls to: nikolay\n",
      "22/05/07 05:11:43 INFO SecurityManager: Changing modify acls to: nikolay\n",
      "22/05/07 05:11:43 INFO SecurityManager: Changing view acls groups to: \n",
      "22/05/07 05:11:43 INFO SecurityManager: Changing modify acls groups to: \n",
      "22/05/07 05:11:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(nikolay); groups with view permissions: Set(); users  with modify permissions: Set(nikolay); groups with modify permissions: Set()\n",
      "22/05/07 05:11:44 INFO Utils: Successfully started service 'sparkDriver' on port 39529.\n",
      "22/05/07 05:11:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "22/05/07 05:11:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "22/05/07 05:11:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "22/05/07 05:11:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "22/05/07 05:11:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "22/05/07 05:11:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a1dbf9fc-d27a-404b-87cd-0195443cb772\n",
      "22/05/07 05:11:44 INFO MemoryStore: MemoryStore started with capacity 11.8 GiB\n",
      "22/05/07 05:11:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "22/05/07 05:11:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "22/05/07 05:11:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.43.45:4040\n",
      "22/05/07 05:11:45 ERROR SparkContext: Failed to add postgresql-42.2.5.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /home/nikolay/Projects/tinkoff-invest/Collect_data/postgresql-42.2.5.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1935)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:1990)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:503)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:503)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:503)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/05/07 05:11:45 INFO SparkContext: Added file file:///home/nikolay/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar at file:///home/nikolay/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar with timestamp 1651889503507\n",
      "22/05/07 05:11:45 INFO Utils: Copying /home/nikolay/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar to /tmp/spark-14817f4d-645e-491e-8773-0c690173333a/userFiles-b91e085c-ac15-48b3-ac82-495cbdfcc10f/org.postgresql_postgresql-42.2.5.jar\n",
      "22/05/07 05:11:45 INFO Executor: Starting executor ID driver on host 192.168.43.45\n",
      "22/05/07 05:11:45 INFO Executor: Fetching file:///home/nikolay/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar with timestamp 1651889503507\n",
      "22/05/07 05:11:45 INFO Utils: /home/nikolay/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar has been previously copied to /tmp/spark-14817f4d-645e-491e-8773-0c690173333a/userFiles-b91e085c-ac15-48b3-ac82-495cbdfcc10f/org.postgresql_postgresql-42.2.5.jar\n",
      "22/05/07 05:11:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34139.\n",
      "22/05/07 05:11:45 INFO NettyBlockTransferService: Server created on 192.168.43.45:34139\n",
      "22/05/07 05:11:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "22/05/07 05:11:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.43.45, 34139, None)\n",
      "22/05/07 05:11:45 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.43.45:34139 with 11.8 GiB RAM, BlockManagerId(driver, 192.168.43.45, 34139, None)\n",
      "22/05/07 05:11:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.43.45, 34139, None)\n",
      "22/05/07 05:11:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.43.45, 34139, None)\n",
      "22/05/07 05:11:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "22/05/07 05:11:46 INFO SharedState: Warehouse path is 'file:/home/nikolay/Projects/tinkoff-invest/Collect_data/spark-warehouse'.\n",
      "22/05/07 05:11:47 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.43.45:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>tinkof-invest</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9c21a0bbd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# initialise sparkContext\n",
    "\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.2.5.jar\")\\\n",
    "    .config(\"spark.jars.packages\",\"org.postgresql:postgresql:42.2.5\")\\\n",
    "    .config('spark.driver.maxResultSize','20g').config(\"spark.executor.memory\", \"20g\")\\\n",
    "    .config(\"spark.driver.memory\",\"20g\").config(\"spark.executor.memory\",\"20g\")\\\n",
    "    .config(\"spark.shuffle.file.buffer\",'40').config(\"spark.scheduler.listenerbus.eventqueue.capacity\",\"10000000\")\\\n",
    "    .master(\"local\").appName(\"tinkof-invest\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60de3e23-86c8-47ff-8925-c2add1e77927",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_info = spark.createDataFrame(pd.read_parquet('Shares.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac19a4f-b50e-497b-b48a-d765db7aaf5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[figi: string, ticker: string, class_code: string, isin: string, lot: bigint, currency: string, klong: decimal(9,8), kshort: decimal(9,8), dlong: decimal(9,8), dshort: decimal(9,8), dlong_min: decimal(9,8), dshort_min: decimal(9,8), short_enabled_flag: decimal(9,8), name: string, exchange: string, ipo_date: timestamp, issue_size: bigint, nominal: decimal(13,9), trading_status: bigint, otc_flag: boolean, buy_available_flag: boolean, sell_available_flag: boolean, div_yield_flag: boolean, share_type: bigint, min_price_increment: decimal(10,8), api_trade_available_flag: boolean, uid: string, real_exchange: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shares_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec7e39f9-bd3d-4706-b8d9-e1bc460f468b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x7f5ee844c350>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shares_info.write.format('jdbc').option(\"url\",\"jdbc:postgresql://localhost:5432/test_db\")\\\n",
    "    .option(\"table\",'shares_info').option(\"user\",'nikolay').option(\"password\",\"Andone1\")\\\n",
    "    .option(\"mode\",\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ed94fc-aaa5-4d03-a083-0bfa729785be",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"overwrite\"\n",
    "url = \"jdbc:postgresql://localhost:5432/shares\"\n",
    "properties = {\"user\": \"nikolay\",\"password\": \"Andone1\",\"driver\": \"org.postgresql.Driver\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b52b5a1-fc4f-4c55-ab00-e500c4216222",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candles_day = pd.read_parquet('Shares_market_data2000_2022.parquet')\n",
    "# all_candles_day.write.format('jdbc').option(\"url\",\"jdbc:postgresql://localhost:5432/test_db\")\\\n",
    "#     .option(\"table\",'candles_day').option(\"user\",'nikolay').option(\"password\",\"Andone1\")\\\n",
    "#     .option(\"mode\",\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5d7a94-5d53-4543-8185-4818ea2178db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_now = pd.read_parquet(\"Shares_market_data2022_now.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16b3bc19-6f13-4588-8cb6-0937e862225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([all_candles_day,data_now])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726ee064-13c0-4d9a-ab1d-dbad5524658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = spark.createDataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3452d8e8-5674-406c-925d-fa0401097f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "all_data.write.jdbc(url=url, table=\"candles_day\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e983e6b7-d261-492a-86ac-5541d85331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shares_info.write.jdbc(url=url, table=\"shares_info\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75779e4-2c33-47cd-9665-7322fd486e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "print('123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bae3e0-8540-4c51-b4ad-32bbf51d2c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
